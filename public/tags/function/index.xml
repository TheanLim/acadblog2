<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Function on Thean C. Lim</title>
    <link>/tags/function/</link>
    <description>Recent content in Function on Thean C. Lim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Thean Cheat Lim</copyright>
    <lastBuildDate>Thu, 19 Apr 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/function/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Clustering: k-means, k-means &#43;&#43; and gganimate</title>
      <link>/post/clustering-k-means-k-means-and-gganimate/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/clustering-k-means-k-means-and-gganimate/</guid>
      <description>IntroductionThe ProblemK-means ClusteringImplementationData Simulation and VisualizationK-means ++ ClusteringImplementationsVisualizationChoosing K - the Elbow MethodNext StepReferencesIntroductionClustering methods attempt to group object based on the similarities of the objects. For example, one can group their customers into several clusters so that one can aim a specific way of marketing to each type of customers.
Since there is no preexisting group labels for each cluster, we can never know the accuracy of clustering methods unless using a simulated data.</description>
    </item>
    
    <item>
      <title>Cross Validation Function for Classifier</title>
      <link>/post/cross-validation-function-for-classifier/</link>
      <pubDate>Mon, 16 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/cross-validation-function-for-classifier/</guid>
      <description>IntroductionSelf-defined functionscvclassifierArguments and ValuesImplementationsclasspred.cvArguments and ValuesImplementationsSimulation, Errors and KNN BoundariesExample 1Simulate DataTraining and Testing ErrorsCross ValidationDecision BoundariesExample 2Simulate DataTraining and Testing ErrorsCross ValidationDecision BoundariesNext StepsIntroductionAs mentioned in the previous post, the natural step after creating a KNN classifier is to define another function that can be used for cross-validation (CV).</description>
    </item>
    
    <item>
      <title>K Nearest Neighbour Classsifier (self-written function)</title>
      <link>/post/k-nearest-neighbour-classifier-self-written-function/</link>
      <pubDate>Sun, 15 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/k-nearest-neighbour-classifier-self-written-function/</guid>
      <description>IntroductionSelf-defined KNN ClassifierSimulation, errors and KNN BoundarySimulate dataTraining and Testing ErrorsDecision boundariesNext stepsIntroductionThe K-nearest neighbors (KNN) classifier works by indentifying \(K\) (a positive integer) training data points that are closest (defined by Euclidean distance) to a test observation \(x_0\) and calculate the conditional probability of \(x_0\) belongs to class \(j\). The conditional probability equals to the fraction of the \(K\) training data points that belongs to class \(j\).</description>
    </item>
    
  </channel>
</rss>