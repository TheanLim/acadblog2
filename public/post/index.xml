<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Thean C. Lim</title>
    <link>/post/</link>
    <description>Recent content in Posts on Thean C. Lim</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Thean Cheat Lim</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0600</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Clustering: k-means, k-means &#43;&#43; and gganimate</title>
      <link>/post/clustering-k-means-k-means-and-gganimate/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/clustering-k-means-k-means-and-gganimate/</guid>
      <description>IntroductionThe ProblemK-means ClusteringImplementationData Simulation and VisualizationK-means ++ ClusteringImplementationsVisualizationChoosing K - the Elbow MethodNext StepReferencesIntroductionClustering methods attempt to group object based on the similarities of the objects. For example, one can group their customers into several clusters so that one can aim a specific way of marketing to each type of customers.
Since there is no preexisting group labels for each cluster, we can never know the accuracy of clustering methods unless using a simulated data.</description>
    </item>
    
    <item>
      <title>gganimate: Animations with ggplot2</title>
      <link>/post/gganimate-animations-with-ggplot2/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/gganimate-animations-with-ggplot2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cross Validation Function for Classifier</title>
      <link>/post/cross-validation-function-for-classifier/</link>
      <pubDate>Mon, 16 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/cross-validation-function-for-classifier/</guid>
      <description>IntroductionSelf-defined functionscvclassifierArguments and ValuesImplementationsclasspred.cvArguments and ValuesImplementationsSimulation, Errors and KNN BoundariesExample 1Simulate DataTraining and Testing ErrorsCross ValidationDecision BoundariesExample 2Simulate DataTraining and Testing ErrorsCross ValidationDecision BoundariesNext StepsIntroductionAs mentioned in the previous post, the natural step after creating a KNN classifier is to define another function that can be used for cross-validation (CV).</description>
    </item>
    
    <item>
      <title>K Nearest Neighbour Classsifier (self-written function)</title>
      <link>/post/k-nearest-neighbour-classifier-self-written-function/</link>
      <pubDate>Sun, 15 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/k-nearest-neighbour-classifier-self-written-function/</guid>
      <description>IntroductionSelf-defined KNN ClassifierSimulation, errors and KNN BoundarySimulate dataTraining and Testing ErrorsDecision boundariesNext stepsIntroductionThe K-nearest neighbors (KNN) classifier works by indentifying \(K\) (a positive integer) training data points that are closest (defined by Euclidean distance) to a test observation \(x_0\) and calculate the conditional probability of \(x_0\) belongs to class \(j\). The conditional probability equals to the fraction of the \(K\) training data points that belongs to class \(j\).</description>
    </item>
    
    <item>
      <title>Linear Regression Simulation Study</title>
      <link>/post/linear-regression-simulation-study/</link>
      <pubDate>Sat, 14 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/linear-regression-simulation-study/</guid>
      <description>TheoriesModel AssumptionsOrdinary Least Square Estimator (OLS) of \(\beta\)Estimator of \(\sigma^2\)SimulationVisualize data pointsEstimating \(\beta\) and \(\epsilon\)Visualize fitted planeIn general, linear regression is a linear approach of modelling the relationship of a numerical response (dependent) variable and one or more explanatory (independent) variables.
TheoriesModel AssumptionsLet:
\(y_i \in \mathbb{R}^n\) as the measured response for the \(i\)th subject.</description>
    </item>
    
    <item>
      <title>Hello World: Blogging using Rmd</title>
      <link>/post/hello-world-blogging-using-rmd/</link>
      <pubDate>Thu, 12 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/hello-world-blogging-using-rmd/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>